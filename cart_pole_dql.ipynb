{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJ7lFSAqNv8iqZ4L8FF2LX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbkW0SQmQcTx","executionInfo":{"status":"ok","timestamp":1671589438833,"user_tz":-330,"elapsed":3468,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}},"outputId":"8beaf10c-b893-42a9-87f2-43c7fbeecd03"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n","Requirement already satisfied: gym==0.19 in /usr/local/lib/python3.8/dist-packages (0.19.0)\n","Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.9.0)\n","Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.8/dist-packages (1.0.5)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.19) (1.5.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.19) (1.21.6)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (5.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.12.7)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n"]}],"source":["!pip install tensorflow gym==0.19 keras keras-rl2"]},{"cell_type":"code","source":["import gym \n","import random "],"metadata":{"id":"SHeR15ZCQh8e","executionInfo":{"status":"ok","timestamp":1671589438833,"user_tz":-330,"elapsed":6,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["env=gym.make('CartPole-v0')\n","states=env.observation_space.shape[0]\n","actions=env.action_space.n"],"metadata":{"id":"mnPu3gb9Ro3W","executionInfo":{"status":"ok","timestamp":1671589438834,"user_tz":-330,"elapsed":6,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install colabgymrender\n","!pip install imageio==2.4.1\n","!apt-get install python-opengl -y\n","\n","!apt-get install x11-utils > /dev/null 2>&1 \n","!pip install pyglet > /dev/null 2>&1 \n","!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","\n","!apt install xvfb -y\n","\n","!pip install pyvirtualdisplay\n","\n","!pip install piglet\n","\n","!pip install pyglet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E2tKQMQPR1J-","executionInfo":{"status":"ok","timestamp":1671589473516,"user_tz":-330,"elapsed":34687,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}},"outputId":"87225f4e-b3e9-4314-8c83-28bc0b75bc03"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: colabgymrender in /usr/local/lib/python3.8/dist-packages (1.1.0)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.8/dist-packages (from colabgymrender) (0.2.3.5)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.8/dist-packages (from moviepy->colabgymrender) (4.64.1)\n","Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.8/dist-packages (from moviepy->colabgymrender) (2.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from moviepy->colabgymrender) (1.21.6)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.8/dist-packages (from moviepy->colabgymrender) (4.4.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: imageio==2.4.1 in /usr/local/lib/python3.8/dist-packages (2.4.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio==2.4.1) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from imageio==2.4.1) (1.21.6)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","python-opengl is already the newest version (3.1.0+dfsg-1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.13).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.8/dist-packages (3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: piglet in /usr/local/lib/python3.8/dist-packages (1.0.0)\n","Requirement already satisfied: piglet-templates in /usr/local/lib/python3.8/dist-packages (from piglet) (1.3.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (3.0.9)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (22.1.0)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (2.0.1)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse->piglet-templates->piglet) (0.38.4)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyglet in /usr/local/lib/python3.8/dist-packages (1.5.27)\n"]}]},{"cell_type":"code","source":["import random\n","from gym.wrappers import Monitor\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from IPython import display as ipythondisplay\n","\n","# Google Colab needs to render the environment to a virtual display\n","# we will record this as a video and play it after the training has finished\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[-1]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"metadata":{"id":"V_h-hGp9Ws5h","executionInfo":{"status":"ok","timestamp":1671589473516,"user_tz":-330,"elapsed":19,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!pip uninstall pyglet\n","!pip install pyglet==1.5.27"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pwANxwJgZLTA","executionInfo":{"status":"ok","timestamp":1671589545064,"user_tz":-330,"elapsed":71565,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}},"outputId":"dafc37fc-a097-44bc-eb29-074cd24cb1a3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: pyglet 1.5.27\n","Uninstalling pyglet-1.5.27:\n","  Would remove:\n","    /usr/local/lib/python3.8/dist-packages/pyglet-1.5.27.dist-info/*\n","    /usr/local/lib/python3.8/dist-packages/pyglet/*\n","Proceed (y/n)? y\n","  Successfully uninstalled pyglet-1.5.27\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyglet==1.5.27\n","  Using cached pyglet-1.5.27-py3-none-any.whl (1.1 MB)\n","Installing collected packages: pyglet\n","Successfully installed pyglet-1.5.27\n"]}]},{"cell_type":"code","source":["from colabgymrender.recorder import Recorder\n","\n","directory='./video'\n","\n","\n","for episode in range(5):\n","  \n","  state = env.reset() \n","  \n","  done=False \n","  score =0 \n","  #env=Recorder(env,directory)\n","  while not done:\n","    action=random.choice([0,1])\n","    n_state,reward,done,info = env.step(action)\n","    score+=reward \n","  #env.play()\n","  print(\"Episodes:\",episode,\" Score:\",score)\n","#display.clear_output(wait=True)\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uOzyOaVgSFdR","executionInfo":{"status":"ok","timestamp":1671589556227,"user_tz":-330,"elapsed":412,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}},"outputId":"52db7a87-b19d-4c96-b58f-e4b4314e250b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Episodes: 0  Score: 30.0\n","Episodes: 1  Score: 33.0\n","Episodes: 2  Score: 16.0\n","Episodes: 3  Score: 34.0\n","Episodes: 4  Score: 32.0\n"]}]},{"cell_type":"code","source":["import numpy as np \n","from tensorflow.keras.models import Sequential \n","from tensorflow.keras.layers import Dense,Flatten, Convolution2D\n","from tensorflow.keras.optimizers import Adam"],"metadata":{"id":"kbCXSfFPSMRL","executionInfo":{"status":"ok","timestamp":1671589563776,"user_tz":-330,"elapsed":2558,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def build_model(states,action):\n","  model=Sequential()\n","  model.add(Flatten(input_shape=(1,states)))\n","  model.add(Dense(24,activation='relu'))\n","  model.add(Dense(24,activation='relu'))\n","  model.add(Dense(actions,activation='linear'))\n","  return model"],"metadata":{"id":"_jW9vsa8a7SC","executionInfo":{"status":"ok","timestamp":1671589574193,"user_tz":-330,"elapsed":517,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["model=build_model(states,actions)"],"metadata":{"id":"HiHVLTl2bcQe","executionInfo":{"status":"ok","timestamp":1671589574193,"user_tz":-330,"elapsed":1,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sP1C-tiwbkZK","executionInfo":{"status":"ok","timestamp":1671589574775,"user_tz":-330,"elapsed":7,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}},"outputId":"a2e23531-d717-42ff-eeb8-4321737aab48"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_1 (Flatten)         (None, 4)                 0         \n","                                                                 \n"," dense_3 (Dense)             (None, 24)                120       \n","                                                                 \n"," dense_4 (Dense)             (None, 24)                600       \n","                                                                 \n"," dense_5 (Dense)             (None, 2)                 50        \n","                                                                 \n","=================================================================\n","Total params: 770\n","Trainable params: 770\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from rl.agents import DQNAgent\n","from rl.memory import SequentialMemory \n","from rl.policy import BoltzmannQPolicy"],"metadata":{"id":"XPUSBKQEbl14","executionInfo":{"status":"ok","timestamp":1671589574775,"user_tz":-330,"elapsed":2,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"Vln6XwPqdOQT"}},{"cell_type":"code","source":["def build_agent(model,actions):\n","  policy = BoltzmannQPolicy()\n","  memory = SequentialMemory(limit=50000,window_length=1)\n","  dqn = DQNAgent(model=model,memory=memory,policy=policy,nb_actions=actions,nb_steps_warmup=10,target_model_update=1e-2)\n","  return dqn"],"metadata":{"id":"zvvM5Za_bw1u","executionInfo":{"status":"ok","timestamp":1671589576606,"user_tz":-330,"elapsed":568,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["dqn = build_agent(model,actions)\n","dqn.compile(Adam(lr=1e-4),metrics=['mae'])\n","dqn.fit(env,nb_steps=10000,visualize=False,verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OnFKpdvkcTGV","executionInfo":{"status":"ok","timestamp":1671589672179,"user_tz":-330,"elapsed":95575,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}},"outputId":"a8df8625-126e-4e3d-f723-adbdfe2e423a"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training for 10000 steps ...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n","\n","WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n","\n"]},{"output_type":"stream","name":"stdout","text":["   15/10000: episode: 1, duration: 1.080s, episode steps:  15, steps per second:  14, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.586483, mae: 0.633133, mean_q: 0.054173\n","   37/10000: episode: 2, duration: 0.214s, episode steps:  22, steps per second: 103, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.600445, mae: 0.630490, mean_q: 0.067853\n","   54/10000: episode: 3, duration: 0.173s, episode steps:  17, steps per second:  98, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.235 [0.000, 1.000],  loss: 0.558230, mae: 0.581625, mean_q: 0.067712\n","   66/10000: episode: 4, duration: 0.111s, episode steps:  12, steps per second: 108, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.516043, mae: 0.565932, mean_q: 0.094708\n","   79/10000: episode: 5, duration: 0.128s, episode steps:  13, steps per second: 102, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.509860, mae: 0.586314, mean_q: 0.134730\n","   91/10000: episode: 6, duration: 0.119s, episode steps:  12, steps per second: 101, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.502020, mae: 0.594590, mean_q: 0.170951\n","  142/10000: episode: 7, duration: 0.435s, episode steps:  51, steps per second: 117, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 0.490930, mae: 0.578929, mean_q: 0.202531\n","  161/10000: episode: 8, duration: 0.187s, episode steps:  19, steps per second: 101, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.498549, mae: 0.590568, mean_q: 0.247723\n","  180/10000: episode: 9, duration: 0.166s, episode steps:  19, steps per second: 115, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.737 [0.000, 1.000],  loss: 0.509829, mae: 0.604750, mean_q: 0.286377\n","  203/10000: episode: 10, duration: 0.226s, episode steps:  23, steps per second: 102, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.517739, mae: 0.609281, mean_q: 0.309985\n","  216/10000: episode: 11, duration: 0.122s, episode steps:  13, steps per second: 106, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.538236, mae: 0.629625, mean_q: 0.368488\n","  237/10000: episode: 12, duration: 0.199s, episode steps:  21, steps per second: 105, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.526114, mae: 0.624293, mean_q: 0.387782\n","  262/10000: episode: 13, duration: 0.253s, episode steps:  25, steps per second:  99, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.640 [0.000, 1.000],  loss: 0.517878, mae: 0.624808, mean_q: 0.422492\n","  280/10000: episode: 14, duration: 0.170s, episode steps:  18, steps per second: 106, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.510585, mae: 0.622679, mean_q: 0.455688\n","  296/10000: episode: 15, duration: 0.157s, episode steps:  16, steps per second: 102, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.518274, mae: 0.635374, mean_q: 0.502924\n","  313/10000: episode: 16, duration: 0.167s, episode steps:  17, steps per second: 102, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.522086, mae: 0.641659, mean_q: 0.525302\n","  331/10000: episode: 17, duration: 0.174s, episode steps:  18, steps per second: 103, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  loss: 0.497157, mae: 0.634459, mean_q: 0.587199\n","  346/10000: episode: 18, duration: 0.137s, episode steps:  15, steps per second: 110, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.503874, mae: 0.659783, mean_q: 0.621666\n","  362/10000: episode: 19, duration: 0.168s, episode steps:  16, steps per second:  95, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.188 [0.000, 1.000],  loss: 0.516816, mae: 0.670092, mean_q: 0.654002\n","  377/10000: episode: 20, duration: 0.131s, episode steps:  15, steps per second: 114, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.505962, mae: 0.690212, mean_q: 0.703348\n","  387/10000: episode: 21, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.496727, mae: 0.694266, mean_q: 0.701122\n","  410/10000: episode: 22, duration: 0.227s, episode steps:  23, steps per second: 101, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.739 [0.000, 1.000],  loss: 0.492348, mae: 0.711868, mean_q: 0.787596\n","  442/10000: episode: 23, duration: 0.294s, episode steps:  32, steps per second: 109, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.719 [0.000, 1.000],  loss: 0.490543, mae: 0.744087, mean_q: 0.870311\n","  468/10000: episode: 24, duration: 0.246s, episode steps:  26, steps per second: 106, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.493995, mae: 0.771621, mean_q: 0.971085\n","  480/10000: episode: 25, duration: 0.127s, episode steps:  12, steps per second:  95, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.523863, mae: 0.813093, mean_q: 1.033580\n","  502/10000: episode: 26, duration: 0.199s, episode steps:  22, steps per second: 111, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.497333, mae: 0.829622, mean_q: 1.093991\n","  513/10000: episode: 27, duration: 0.111s, episode steps:  11, steps per second:  99, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.471240, mae: 0.834484, mean_q: 1.107552\n","  525/10000: episode: 28, duration: 0.121s, episode steps:  12, steps per second:  99, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.498138, mae: 0.869996, mean_q: 1.171633\n","  545/10000: episode: 29, duration: 0.185s, episode steps:  20, steps per second: 108, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.505293, mae: 0.888026, mean_q: 1.262705\n","  575/10000: episode: 30, duration: 0.291s, episode steps:  30, steps per second: 103, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.509273, mae: 0.935126, mean_q: 1.288352\n","  586/10000: episode: 31, duration: 0.114s, episode steps:  11, steps per second:  96, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.505753, mae: 0.974031, mean_q: 1.412995\n","  601/10000: episode: 32, duration: 0.144s, episode steps:  15, steps per second: 104, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.512833, mae: 1.009645, mean_q: 1.508119\n","  611/10000: episode: 33, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.547394, mae: 1.010607, mean_q: 1.548265\n","  620/10000: episode: 34, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.550278, mae: 1.035213, mean_q: 1.615076\n","  637/10000: episode: 35, duration: 0.159s, episode steps:  17, steps per second: 107, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.573969, mae: 1.037639, mean_q: 1.560486\n","  651/10000: episode: 36, duration: 0.133s, episode steps:  14, steps per second: 106, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.577345, mae: 1.085356, mean_q: 1.590133\n","  660/10000: episode: 37, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.587350, mae: 1.123210, mean_q: 1.739655\n","  677/10000: episode: 38, duration: 0.174s, episode steps:  17, steps per second:  98, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.609559, mae: 1.138031, mean_q: 1.717713\n","  688/10000: episode: 39, duration: 0.104s, episode steps:  11, steps per second: 106, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.635022, mae: 1.166707, mean_q: 1.798054\n","  701/10000: episode: 40, duration: 0.128s, episode steps:  13, steps per second: 101, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.569912, mae: 1.151375, mean_q: 1.777387\n","  715/10000: episode: 41, duration: 0.150s, episode steps:  14, steps per second:  93, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.622788, mae: 1.210860, mean_q: 1.959741\n","  735/10000: episode: 42, duration: 0.176s, episode steps:  20, steps per second: 114, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.665792, mae: 1.256766, mean_q: 1.980629\n","  760/10000: episode: 43, duration: 0.221s, episode steps:  25, steps per second: 113, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.729936, mae: 1.304292, mean_q: 2.084453\n","  770/10000: episode: 44, duration: 0.117s, episode steps:  10, steps per second:  86, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.586291, mae: 1.285443, mean_q: 2.059777\n","  806/10000: episode: 45, duration: 0.338s, episode steps:  36, steps per second: 107, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 0.686808, mae: 1.345592, mean_q: 2.163306\n","  822/10000: episode: 46, duration: 0.143s, episode steps:  16, steps per second: 112, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.699311, mae: 1.413052, mean_q: 2.285800\n","  843/10000: episode: 47, duration: 0.211s, episode steps:  21, steps per second: 100, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.729817, mae: 1.472825, mean_q: 2.422288\n","  852/10000: episode: 48, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.877806, mae: 1.532782, mean_q: 2.508526\n","  864/10000: episode: 49, duration: 0.114s, episode steps:  12, steps per second: 106, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.884277, mae: 1.502494, mean_q: 2.440532\n","  879/10000: episode: 50, duration: 0.138s, episode steps:  15, steps per second: 109, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.809755, mae: 1.534601, mean_q: 2.545920\n","  895/10000: episode: 51, duration: 0.150s, episode steps:  16, steps per second: 106, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.723310, mae: 1.542918, mean_q: 2.541344\n","  907/10000: episode: 52, duration: 0.108s, episode steps:  12, steps per second: 111, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.862674, mae: 1.602101, mean_q: 2.607579\n","  923/10000: episode: 53, duration: 0.144s, episode steps:  16, steps per second: 111, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.933656, mae: 1.653515, mean_q: 2.688495\n","  944/10000: episode: 54, duration: 0.220s, episode steps:  21, steps per second:  95, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.742782, mae: 1.647406, mean_q: 2.771037\n","  955/10000: episode: 55, duration: 0.114s, episode steps:  11, steps per second:  96, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.857769, mae: 1.717942, mean_q: 2.883820\n","  964/10000: episode: 56, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.906349, mae: 1.750033, mean_q: 2.955131\n","  990/10000: episode: 57, duration: 0.226s, episode steps:  26, steps per second: 115, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.654 [0.000, 1.000],  loss: 0.968749, mae: 1.800718, mean_q: 3.069265\n"," 1013/10000: episode: 58, duration: 0.213s, episode steps:  23, steps per second: 108, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.696 [0.000, 1.000],  loss: 0.899046, mae: 1.819356, mean_q: 3.031108\n"," 1022/10000: episode: 59, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.767768, mae: 1.798079, mean_q: 3.020752\n"," 1032/10000: episode: 60, duration: 0.093s, episode steps:  10, steps per second: 107, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.793817, mae: 1.844418, mean_q: 3.133674\n"," 1043/10000: episode: 61, duration: 0.113s, episode steps:  11, steps per second:  98, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.691625, mae: 1.836390, mean_q: 3.190164\n"," 1054/10000: episode: 62, duration: 0.108s, episode steps:  11, steps per second: 102, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.877829, mae: 1.893725, mean_q: 3.283809\n"," 1071/10000: episode: 63, duration: 0.149s, episode steps:  17, steps per second: 114, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 1.001855, mae: 1.983270, mean_q: 3.379952\n"," 1081/10000: episode: 64, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.080192, mae: 2.061715, mean_q: 3.533393\n"," 1097/10000: episode: 65, duration: 0.138s, episode steps:  16, steps per second: 116, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.994145, mae: 2.040578, mean_q: 3.531916\n"," 1109/10000: episode: 66, duration: 0.116s, episode steps:  12, steps per second: 104, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 1.202366, mae: 2.151613, mean_q: 3.654712\n"," 1121/10000: episode: 67, duration: 0.123s, episode steps:  12, steps per second:  97, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 1.224156, mae: 2.204261, mean_q: 3.740073\n"," 1133/10000: episode: 68, duration: 0.111s, episode steps:  12, steps per second: 108, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 1.251191, mae: 2.149243, mean_q: 3.618470\n"," 1155/10000: episode: 69, duration: 0.219s, episode steps:  22, steps per second: 100, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 0.917259, mae: 2.194531, mean_q: 3.822437\n"," 1169/10000: episode: 70, duration: 0.121s, episode steps:  14, steps per second: 116, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 1.230898, mae: 2.259426, mean_q: 3.888380\n"," 1183/10000: episode: 71, duration: 0.144s, episode steps:  14, steps per second:  97, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 1.128127, mae: 2.242559, mean_q: 3.879377\n"," 1200/10000: episode: 72, duration: 0.162s, episode steps:  17, steps per second: 105, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 1.126325, mae: 2.304507, mean_q: 3.991420\n"," 1226/10000: episode: 73, duration: 0.245s, episode steps:  26, steps per second: 106, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.343598, mae: 2.395064, mean_q: 4.100333\n"," 1243/10000: episode: 74, duration: 0.165s, episode steps:  17, steps per second: 103, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 1.252166, mae: 2.412385, mean_q: 4.166391\n"," 1257/10000: episode: 75, duration: 0.133s, episode steps:  14, steps per second: 105, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.516904, mae: 2.504654, mean_q: 4.279807\n"," 1268/10000: episode: 76, duration: 0.104s, episode steps:  11, steps per second: 106, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.426863, mae: 2.482396, mean_q: 4.178399\n"," 1281/10000: episode: 77, duration: 0.124s, episode steps:  13, steps per second: 104, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 1.579192, mae: 2.559088, mean_q: 4.356099\n"," 1300/10000: episode: 78, duration: 0.173s, episode steps:  19, steps per second: 110, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 1.656437, mae: 2.616189, mean_q: 4.432591\n"," 1312/10000: episode: 79, duration: 0.116s, episode steps:  12, steps per second: 104, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.752401, mae: 2.700996, mean_q: 4.569742\n"," 1323/10000: episode: 80, duration: 0.111s, episode steps:  11, steps per second:  99, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.747107, mae: 2.674168, mean_q: 4.459033\n"," 1341/10000: episode: 81, duration: 0.161s, episode steps:  18, steps per second: 112, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 1.691707, mae: 2.686532, mean_q: 4.536958\n"," 1359/10000: episode: 82, duration: 0.180s, episode steps:  18, steps per second: 100, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 1.190930, mae: 2.625825, mean_q: 4.491426\n"," 1382/10000: episode: 83, duration: 0.222s, episode steps:  23, steps per second: 104, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 1.662359, mae: 2.744798, mean_q: 4.697381\n"," 1407/10000: episode: 84, duration: 0.231s, episode steps:  25, steps per second: 108, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.680 [0.000, 1.000],  loss: 1.795681, mae: 2.840406, mean_q: 4.831615\n"," 1419/10000: episode: 85, duration: 0.115s, episode steps:  12, steps per second: 104, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 1.624956, mae: 2.849758, mean_q: 4.797926\n"," 1428/10000: episode: 86, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 1.510624, mae: 2.804567, mean_q: 4.823790\n"," 1437/10000: episode: 87, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.450962, mae: 2.803240, mean_q: 4.809685\n"," 1457/10000: episode: 88, duration: 0.201s, episode steps:  20, steps per second: 100, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 2.061406, mae: 2.938397, mean_q: 4.945123\n"," 1467/10000: episode: 89, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.467983, mae: 2.917164, mean_q: 5.004770\n"," 1492/10000: episode: 90, duration: 0.225s, episode steps:  25, steps per second: 111, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 1.704627, mae: 2.978724, mean_q: 5.064985\n"," 1502/10000: episode: 91, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.202600, mae: 2.924085, mean_q: 5.081488\n"," 1519/10000: episode: 92, duration: 0.145s, episode steps:  17, steps per second: 117, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 1.810891, mae: 3.046868, mean_q: 5.240755\n"," 1533/10000: episode: 93, duration: 0.145s, episode steps:  14, steps per second:  97, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 1.695396, mae: 3.060728, mean_q: 5.230803\n"," 1543/10000: episode: 94, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.801957, mae: 3.095325, mean_q: 5.307502\n"," 1574/10000: episode: 95, duration: 0.299s, episode steps:  31, steps per second: 104, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.603205, mae: 3.098902, mean_q: 5.344562\n"," 1584/10000: episode: 96, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.441473, mae: 3.121727, mean_q: 5.435013\n"," 1596/10000: episode: 97, duration: 0.110s, episode steps:  12, steps per second: 109, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.981688, mae: 3.249895, mean_q: 5.598658\n"," 1617/10000: episode: 98, duration: 0.184s, episode steps:  21, steps per second: 114, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.210694, mae: 3.305650, mean_q: 5.619308\n"," 1626/10000: episode: 99, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.104755, mae: 3.163588, mean_q: 5.561667\n"," 1644/10000: episode: 100, duration: 0.168s, episode steps:  18, steps per second: 107, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.962217, mae: 3.320224, mean_q: 5.676375\n"," 1672/10000: episode: 101, duration: 0.247s, episode steps:  28, steps per second: 113, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.704419, mae: 3.301621, mean_q: 5.686778\n"," 1711/10000: episode: 102, duration: 0.334s, episode steps:  39, steps per second: 117, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 1.805633, mae: 3.379663, mean_q: 5.859578\n"," 1720/10000: episode: 103, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 2.125512, mae: 3.496346, mean_q: 5.988470\n"," 1733/10000: episode: 104, duration: 0.114s, episode steps:  13, steps per second: 114, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.722623, mae: 3.491541, mean_q: 6.076711\n"," 1745/10000: episode: 105, duration: 0.121s, episode steps:  12, steps per second:  99, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.955009, mae: 3.498855, mean_q: 6.030926\n"," 1778/10000: episode: 106, duration: 0.300s, episode steps:  33, steps per second: 110, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 1.802594, mae: 3.543322, mean_q: 6.149586\n"," 1794/10000: episode: 107, duration: 0.145s, episode steps:  16, steps per second: 110, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 2.239479, mae: 3.607043, mean_q: 6.202628\n"," 1803/10000: episode: 108, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 1.792047, mae: 3.585679, mean_q: 6.221606\n"," 1826/10000: episode: 109, duration: 0.219s, episode steps:  23, steps per second: 105, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.952548, mae: 3.626541, mean_q: 6.291153\n"," 1840/10000: episode: 110, duration: 0.129s, episode steps:  14, steps per second: 108, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 2.150444, mae: 3.647375, mean_q: 6.294041\n"," 1854/10000: episode: 111, duration: 0.148s, episode steps:  14, steps per second:  95, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 2.364408, mae: 3.724123, mean_q: 6.365784\n"," 1870/10000: episode: 112, duration: 0.162s, episode steps:  16, steps per second:  99, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.188 [0.000, 1.000],  loss: 2.014786, mae: 3.722933, mean_q: 6.396276\n"," 1880/10000: episode: 113, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 2.133460, mae: 3.767247, mean_q: 6.488097\n"," 1897/10000: episode: 114, duration: 0.160s, episode steps:  17, steps per second: 107, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 1.874154, mae: 3.728300, mean_q: 6.529477\n"," 1907/10000: episode: 115, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.291617, mae: 3.886637, mean_q: 6.734727\n"," 1926/10000: episode: 116, duration: 0.168s, episode steps:  19, steps per second: 113, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.746246, mae: 3.759138, mean_q: 6.647606\n"," 1934/10000: episode: 117, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 3.024938, mae: 3.995855, mean_q: 6.824569\n"," 1947/10000: episode: 118, duration: 0.134s, episode steps:  13, steps per second:  97, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 2.257614, mae: 3.876397, mean_q: 6.795620\n"," 1976/10000: episode: 119, duration: 0.273s, episode steps:  29, steps per second: 106, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 2.001184, mae: 3.916362, mean_q: 6.862101\n"," 1999/10000: episode: 120, duration: 0.216s, episode steps:  23, steps per second: 107, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 2.619291, mae: 4.055151, mean_q: 6.966299\n"," 2012/10000: episode: 121, duration: 0.122s, episode steps:  13, steps per second: 107, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.456352, mae: 3.899930, mean_q: 6.893955\n"," 2036/10000: episode: 122, duration: 0.226s, episode steps:  24, steps per second: 106, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.014251, mae: 3.992966, mean_q: 7.024649\n"," 2047/10000: episode: 123, duration: 0.102s, episode steps:  11, steps per second: 107, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 1.562647, mae: 3.947021, mean_q: 7.095269\n"," 2057/10000: episode: 124, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.523306, mae: 3.974501, mean_q: 7.122423\n"," 2074/10000: episode: 125, duration: 0.170s, episode steps:  17, steps per second: 100, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 1.785960, mae: 4.043281, mean_q: 7.204831\n"," 2089/10000: episode: 126, duration: 0.140s, episode steps:  15, steps per second: 107, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 2.328771, mae: 4.144860, mean_q: 7.327526\n"," 2103/10000: episode: 127, duration: 0.144s, episode steps:  14, steps per second:  97, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.833118, mae: 4.094028, mean_q: 7.314914\n"," 2127/10000: episode: 128, duration: 0.220s, episode steps:  24, steps per second: 109, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 2.822833, mae: 4.263889, mean_q: 7.380733\n"," 2149/10000: episode: 129, duration: 0.195s, episode steps:  22, steps per second: 113, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.255942, mae: 4.215424, mean_q: 7.381290\n"," 2163/10000: episode: 130, duration: 0.133s, episode steps:  14, steps per second: 105, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.325516, mae: 4.251664, mean_q: 7.441175\n"," 2205/10000: episode: 131, duration: 0.394s, episode steps:  42, steps per second: 107, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.405 [0.000, 1.000],  loss: 2.021217, mae: 4.246352, mean_q: 7.522204\n"," 2215/10000: episode: 132, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 2.551668, mae: 4.376228, mean_q: 7.713338\n"," 2228/10000: episode: 133, duration: 0.133s, episode steps:  13, steps per second:  98, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 2.789051, mae: 4.400864, mean_q: 7.745242\n"," 2244/10000: episode: 134, duration: 0.141s, episode steps:  16, steps per second: 114, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 2.583665, mae: 4.412581, mean_q: 7.733850\n"," 2257/10000: episode: 135, duration: 0.130s, episode steps:  13, steps per second: 100, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 2.412413, mae: 4.412766, mean_q: 7.697177\n"," 2275/10000: episode: 136, duration: 0.162s, episode steps:  18, steps per second: 111, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.765729, mae: 4.424255, mean_q: 7.695154\n"," 2286/10000: episode: 137, duration: 0.112s, episode steps:  11, steps per second:  98, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.552922, mae: 4.323061, mean_q: 7.702658\n"," 2309/10000: episode: 138, duration: 0.210s, episode steps:  23, steps per second: 110, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.432333, mae: 4.439564, mean_q: 7.805333\n"," 2328/10000: episode: 139, duration: 0.170s, episode steps:  19, steps per second: 112, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 1.870982, mae: 4.397600, mean_q: 7.883803\n"," 2348/10000: episode: 140, duration: 0.190s, episode steps:  20, steps per second: 105, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 2.105430, mae: 4.463611, mean_q: 7.978956\n"," 2363/10000: episode: 141, duration: 0.143s, episode steps:  15, steps per second: 105, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.245098, mae: 4.462222, mean_q: 8.005718\n"," 2382/10000: episode: 142, duration: 0.172s, episode steps:  19, steps per second: 111, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 2.255657, mae: 4.545389, mean_q: 8.135822\n"," 2397/10000: episode: 143, duration: 0.135s, episode steps:  15, steps per second: 111, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 2.085037, mae: 4.502519, mean_q: 8.099500\n"," 2412/10000: episode: 144, duration: 0.142s, episode steps:  15, steps per second: 105, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.367144, mae: 4.551778, mean_q: 8.163830\n"," 2429/10000: episode: 145, duration: 0.154s, episode steps:  17, steps per second: 111, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 2.308066, mae: 4.605417, mean_q: 8.251617\n"," 2441/10000: episode: 146, duration: 0.117s, episode steps:  12, steps per second: 102, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 2.535379, mae: 4.672685, mean_q: 8.276822\n"," 2453/10000: episode: 147, duration: 0.114s, episode steps:  12, steps per second: 106, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 2.833282, mae: 4.741023, mean_q: 8.348518\n"," 2469/10000: episode: 148, duration: 0.148s, episode steps:  16, steps per second: 108, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 2.052267, mae: 4.634007, mean_q: 8.323855\n"," 2483/10000: episode: 149, duration: 0.126s, episode steps:  14, steps per second: 111, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 2.434590, mae: 4.706776, mean_q: 8.385717\n"," 2504/10000: episode: 150, duration: 0.197s, episode steps:  21, steps per second: 106, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 2.352449, mae: 4.732915, mean_q: 8.421411\n"," 2534/10000: episode: 151, duration: 0.266s, episode steps:  30, steps per second: 113, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 2.800813, mae: 4.805200, mean_q: 8.492893\n"," 2561/10000: episode: 152, duration: 0.247s, episode steps:  27, steps per second: 110, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.323711, mae: 4.804051, mean_q: 8.541399\n"," 2571/10000: episode: 153, duration: 0.093s, episode steps:  10, steps per second: 107, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 2.905516, mae: 4.867471, mean_q: 8.577432\n"," 2594/10000: episode: 154, duration: 0.216s, episode steps:  23, steps per second: 107, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 2.031596, mae: 4.801702, mean_q: 8.592231\n"," 2607/10000: episode: 155, duration: 0.127s, episode steps:  13, steps per second: 103, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 2.277282, mae: 4.854637, mean_q: 8.674412\n"," 2622/10000: episode: 156, duration: 0.154s, episode steps:  15, steps per second:  98, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.995831, mae: 4.876483, mean_q: 8.804562\n"," 2633/10000: episode: 157, duration: 0.110s, episode steps:  11, steps per second: 100, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.022990, mae: 4.855896, mean_q: 8.785666\n"," 2645/10000: episode: 158, duration: 0.119s, episode steps:  12, steps per second: 101, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.960339, mae: 4.877123, mean_q: 8.868738\n"," 2663/10000: episode: 159, duration: 0.166s, episode steps:  18, steps per second: 109, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.798437, mae: 4.905909, mean_q: 8.960428\n"," 2678/10000: episode: 160, duration: 0.140s, episode steps:  15, steps per second: 107, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.264778, mae: 4.986172, mean_q: 9.017996\n"," 2707/10000: episode: 161, duration: 0.256s, episode steps:  29, steps per second: 113, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.854307, mae: 4.971925, mean_q: 9.049088\n"," 2718/10000: episode: 162, duration: 0.178s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.220393, mae: 5.026418, mean_q: 9.170127\n"," 2743/10000: episode: 163, duration: 0.230s, episode steps:  25, steps per second: 109, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.320 [0.000, 1.000],  loss: 2.011381, mae: 5.037997, mean_q: 9.205644\n"," 2760/10000: episode: 164, duration: 0.174s, episode steps:  17, steps per second:  97, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 1.969233, mae: 5.105550, mean_q: 9.309834\n"," 2785/10000: episode: 165, duration: 0.310s, episode steps:  25, steps per second:  81, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.508554, mae: 5.196250, mean_q: 9.330780\n"," 2796/10000: episode: 166, duration: 0.097s, episode steps:  11, steps per second: 114, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 2.150147, mae: 5.159050, mean_q: 9.317327\n"," 2828/10000: episode: 167, duration: 0.301s, episode steps:  32, steps per second: 106, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  loss: 2.123654, mae: 5.169486, mean_q: 9.411041\n"," 2848/10000: episode: 168, duration: 0.261s, episode steps:  20, steps per second:  77, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 2.308627, mae: 5.248141, mean_q: 9.509322\n"," 2873/10000: episode: 169, duration: 0.375s, episode steps:  25, steps per second:  67, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  loss: 2.097310, mae: 5.229513, mean_q: 9.547516\n"," 2895/10000: episode: 170, duration: 0.227s, episode steps:  22, steps per second:  97, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.514219, mae: 5.353514, mean_q: 9.668567\n"," 2905/10000: episode: 171, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 2.759854, mae: 5.324914, mean_q: 9.582603\n"," 2921/10000: episode: 172, duration: 0.148s, episode steps:  16, steps per second: 108, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 2.376579, mae: 5.318797, mean_q: 9.563222\n"," 2935/10000: episode: 173, duration: 0.208s, episode steps:  14, steps per second:  67, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.326114, mae: 5.293196, mean_q: 9.567667\n"," 2962/10000: episode: 174, duration: 0.351s, episode steps:  27, steps per second:  77, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.630 [0.000, 1.000],  loss: 2.724722, mae: 5.409839, mean_q: 9.729527\n"," 2973/10000: episode: 175, duration: 0.101s, episode steps:  11, steps per second: 109, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 2.986781, mae: 5.442798, mean_q: 9.764776\n"," 2985/10000: episode: 176, duration: 0.121s, episode steps:  12, steps per second:  99, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 2.798280, mae: 5.449246, mean_q: 9.745067\n"," 2999/10000: episode: 177, duration: 0.133s, episode steps:  14, steps per second: 105, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.266867, mae: 5.422291, mean_q: 9.865661\n"," 3015/10000: episode: 178, duration: 0.304s, episode steps:  16, steps per second:  53, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.601691, mae: 5.497766, mean_q: 9.887342\n"," 3037/10000: episode: 179, duration: 0.207s, episode steps:  22, steps per second: 106, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.695867, mae: 5.470527, mean_q: 9.810093\n"," 3064/10000: episode: 180, duration: 0.236s, episode steps:  27, steps per second: 115, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.724658, mae: 5.541739, mean_q: 9.979054\n"," 3088/10000: episode: 181, duration: 0.233s, episode steps:  24, steps per second: 103, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.556973, mae: 5.547753, mean_q: 9.984662\n"," 3107/10000: episode: 182, duration: 0.170s, episode steps:  19, steps per second: 112, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 2.860811, mae: 5.533518, mean_q: 9.916473\n"," 3146/10000: episode: 183, duration: 0.352s, episode steps:  39, steps per second: 111, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.539983, mae: 5.580289, mean_q: 10.053616\n"," 3168/10000: episode: 184, duration: 0.197s, episode steps:  22, steps per second: 112, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 1.925508, mae: 5.563901, mean_q: 10.138780\n"," 3180/10000: episode: 185, duration: 0.120s, episode steps:  12, steps per second: 100, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.745543, mae: 5.620162, mean_q: 10.209262\n"," 3202/10000: episode: 186, duration: 0.216s, episode steps:  22, steps per second: 102, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.916632, mae: 5.601287, mean_q: 10.278642\n"," 3223/10000: episode: 187, duration: 0.188s, episode steps:  21, steps per second: 112, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.063464, mae: 5.636844, mean_q: 10.376118\n"," 3245/10000: episode: 188, duration: 0.207s, episode steps:  22, steps per second: 106, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 3.100145, mae: 5.773655, mean_q: 10.476299\n"," 3271/10000: episode: 189, duration: 0.240s, episode steps:  26, steps per second: 108, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 2.291254, mae: 5.692721, mean_q: 10.391119\n"," 3287/10000: episode: 190, duration: 0.139s, episode steps:  16, steps per second: 115, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 2.585854, mae: 5.740594, mean_q: 10.453238\n"," 3305/10000: episode: 191, duration: 0.168s, episode steps:  18, steps per second: 107, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 2.950119, mae: 5.820964, mean_q: 10.561387\n"," 3333/10000: episode: 192, duration: 0.263s, episode steps:  28, steps per second: 107, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 2.931043, mae: 5.859346, mean_q: 10.622592\n"," 3344/10000: episode: 193, duration: 0.101s, episode steps:  11, steps per second: 109, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.430648, mae: 5.728947, mean_q: 10.674280\n"," 3368/10000: episode: 194, duration: 0.221s, episode steps:  24, steps per second: 109, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.816569, mae: 5.857288, mean_q: 10.651316\n"," 3390/10000: episode: 195, duration: 0.209s, episode steps:  22, steps per second: 105, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.078441, mae: 5.844231, mean_q: 10.737050\n"," 3426/10000: episode: 196, duration: 0.344s, episode steps:  36, steps per second: 105, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.306 [0.000, 1.000],  loss: 2.038347, mae: 5.872118, mean_q: 10.872451\n"," 3441/10000: episode: 197, duration: 0.143s, episode steps:  15, steps per second: 105, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.479429, mae: 5.921167, mean_q: 10.912155\n"," 3453/10000: episode: 198, duration: 0.115s, episode steps:  12, steps per second: 104, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 2.278594, mae: 6.025807, mean_q: 11.179446\n"," 3466/10000: episode: 199, duration: 0.115s, episode steps:  13, steps per second: 113, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 2.405105, mae: 5.963859, mean_q: 11.064970\n"," 3488/10000: episode: 200, duration: 0.193s, episode steps:  22, steps per second: 114, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 1.983344, mae: 5.941318, mean_q: 11.062574\n"," 3513/10000: episode: 201, duration: 0.220s, episode steps:  25, steps per second: 114, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 2.048269, mae: 6.049839, mean_q: 11.234677\n"," 3574/10000: episode: 202, duration: 0.548s, episode steps:  61, steps per second: 111, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.557 [0.000, 1.000],  loss: 2.451267, mae: 6.132437, mean_q: 11.335789\n"," 3597/10000: episode: 203, duration: 0.194s, episode steps:  23, steps per second: 118, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.348 [0.000, 1.000],  loss: 2.491002, mae: 6.195174, mean_q: 11.421824\n"," 3619/10000: episode: 204, duration: 0.200s, episode steps:  22, steps per second: 110, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.232870, mae: 6.317333, mean_q: 11.576234\n"," 3634/10000: episode: 205, duration: 0.163s, episode steps:  15, steps per second:  92, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.219347, mae: 6.227980, mean_q: 11.488556\n"," 3647/10000: episode: 206, duration: 0.130s, episode steps:  13, steps per second: 100, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.128625, mae: 6.254735, mean_q: 11.368536\n"," 3658/10000: episode: 207, duration: 0.115s, episode steps:  11, steps per second:  96, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 2.812076, mae: 6.236929, mean_q: 11.333424\n"," 3676/10000: episode: 208, duration: 0.180s, episode steps:  18, steps per second: 100, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.577102, mae: 6.259062, mean_q: 11.494143\n"," 3697/10000: episode: 209, duration: 0.200s, episode steps:  21, steps per second: 105, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.411988, mae: 6.282492, mean_q: 11.534868\n"," 3711/10000: episode: 210, duration: 0.131s, episode steps:  14, steps per second: 107, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.382509, mae: 6.147818, mean_q: 11.484557\n"," 3723/10000: episode: 211, duration: 0.115s, episode steps:  12, steps per second: 104, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 3.223221, mae: 6.373520, mean_q: 11.682037\n"," 3746/10000: episode: 212, duration: 0.221s, episode steps:  23, steps per second: 104, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.261 [0.000, 1.000],  loss: 3.265126, mae: 6.390378, mean_q: 11.696326\n"," 3771/10000: episode: 213, duration: 0.248s, episode steps:  25, steps per second: 101, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.280 [0.000, 1.000],  loss: 2.804422, mae: 6.389008, mean_q: 11.736737\n"," 3825/10000: episode: 214, duration: 0.482s, episode steps:  54, steps per second: 112, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 2.414937, mae: 6.401406, mean_q: 11.818799\n"," 3838/10000: episode: 215, duration: 0.134s, episode steps:  13, steps per second:  97, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 3.685461, mae: 6.508234, mean_q: 11.907312\n"," 3875/10000: episode: 216, duration: 0.334s, episode steps:  37, steps per second: 111, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.914737, mae: 6.518720, mean_q: 11.986869\n"," 3902/10000: episode: 217, duration: 0.243s, episode steps:  27, steps per second: 111, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.954082, mae: 6.514154, mean_q: 12.043311\n"," 3944/10000: episode: 218, duration: 0.382s, episode steps:  42, steps per second: 110, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.624235, mae: 6.560139, mean_q: 12.194730\n"," 3973/10000: episode: 219, duration: 0.262s, episode steps:  29, steps per second: 111, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.414 [0.000, 1.000],  loss: 2.762479, mae: 6.606599, mean_q: 12.227055\n"," 4000/10000: episode: 220, duration: 0.256s, episode steps:  27, steps per second: 106, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.150776, mae: 6.682988, mean_q: 12.317553\n"," 4014/10000: episode: 221, duration: 0.125s, episode steps:  14, steps per second: 112, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.660945, mae: 6.587107, mean_q: 12.365118\n"," 4029/10000: episode: 222, duration: 0.128s, episode steps:  15, steps per second: 117, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.082108, mae: 6.757577, mean_q: 12.528711\n"," 4044/10000: episode: 223, duration: 0.148s, episode steps:  15, steps per second: 101, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4.557662, mae: 6.855876, mean_q: 12.463881\n"," 4059/10000: episode: 224, duration: 0.134s, episode steps:  15, steps per second: 112, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.823066, mae: 6.599683, mean_q: 12.343000\n"," 4081/10000: episode: 225, duration: 0.246s, episode steps:  22, steps per second:  89, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.197652, mae: 6.781158, mean_q: 12.565942\n"," 4102/10000: episode: 226, duration: 0.180s, episode steps:  21, steps per second: 116, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.766756, mae: 6.779444, mean_q: 12.576358\n"," 4129/10000: episode: 227, duration: 0.255s, episode steps:  27, steps per second: 106, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.194155, mae: 6.883626, mean_q: 12.784822\n"," 4145/10000: episode: 228, duration: 0.142s, episode steps:  16, steps per second: 113, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 2.987655, mae: 6.856611, mean_q: 12.749639\n"," 4185/10000: episode: 229, duration: 0.372s, episode steps:  40, steps per second: 108, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.325 [0.000, 1.000],  loss: 2.888568, mae: 6.909862, mean_q: 12.810289\n"," 4202/10000: episode: 230, duration: 0.165s, episode steps:  17, steps per second: 103, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 3.091521, mae: 6.976521, mean_q: 12.879292\n"," 4256/10000: episode: 231, duration: 0.491s, episode steps:  54, steps per second: 110, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.036389, mae: 6.976571, mean_q: 12.939526\n"," 4270/10000: episode: 232, duration: 0.131s, episode steps:  14, steps per second: 107, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.765820, mae: 6.994631, mean_q: 12.927256\n"," 4293/10000: episode: 233, duration: 0.224s, episode steps:  23, steps per second: 103, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.508778, mae: 6.995887, mean_q: 13.097445\n"," 4310/10000: episode: 234, duration: 0.162s, episode steps:  17, steps per second: 105, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 2.865816, mae: 6.927081, mean_q: 12.978532\n"," 4363/10000: episode: 235, duration: 0.454s, episode steps:  53, steps per second: 117, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.377 [0.000, 1.000],  loss: 2.857537, mae: 7.066362, mean_q: 13.227472\n"," 4389/10000: episode: 236, duration: 0.245s, episode steps:  26, steps per second: 106, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.701456, mae: 7.199737, mean_q: 13.307880\n"," 4440/10000: episode: 237, duration: 0.463s, episode steps:  51, steps per second: 110, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.385263, mae: 7.161548, mean_q: 13.408604\n"," 4481/10000: episode: 238, duration: 0.377s, episode steps:  41, steps per second: 109, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 3.632099, mae: 7.312411, mean_q: 13.570546\n"," 4497/10000: episode: 239, duration: 0.147s, episode steps:  16, steps per second: 109, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 3.757843, mae: 7.329494, mean_q: 13.590092\n"," 4565/10000: episode: 240, duration: 0.625s, episode steps:  68, steps per second: 109, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 3.616547, mae: 7.331543, mean_q: 13.528201\n"," 4580/10000: episode: 241, duration: 0.153s, episode steps:  15, steps per second:  98, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 3.257926, mae: 7.367848, mean_q: 13.553670\n"," 4624/10000: episode: 242, duration: 0.413s, episode steps:  44, steps per second: 107, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 3.042675, mae: 7.328460, mean_q: 13.651901\n"," 4660/10000: episode: 243, duration: 0.326s, episode steps:  36, steps per second: 110, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.552200, mae: 7.360880, mean_q: 13.781368\n"," 4677/10000: episode: 244, duration: 0.160s, episode steps:  17, steps per second: 106, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 3.097827, mae: 7.470378, mean_q: 13.967023\n"," 4687/10000: episode: 245, duration: 0.093s, episode steps:  10, steps per second: 108, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.393529, mae: 7.540183, mean_q: 14.136833\n"," 4726/10000: episode: 246, duration: 0.396s, episode steps:  39, steps per second:  98, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.911202, mae: 7.575483, mean_q: 14.074280\n"," 4762/10000: episode: 247, duration: 0.317s, episode steps:  36, steps per second: 114, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.579947, mae: 7.586588, mean_q: 14.096528\n"," 4789/10000: episode: 248, duration: 0.269s, episode steps:  27, steps per second: 100, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.370 [0.000, 1.000],  loss: 3.102813, mae: 7.579705, mean_q: 14.115694\n"," 4827/10000: episode: 249, duration: 0.380s, episode steps:  38, steps per second: 100, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.420397, mae: 7.643175, mean_q: 14.238528\n"," 4857/10000: episode: 250, duration: 0.294s, episode steps:  30, steps per second: 102, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.863633, mae: 7.679495, mean_q: 14.370859\n"," 4900/10000: episode: 251, duration: 0.390s, episode steps:  43, steps per second: 110, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 3.191989, mae: 7.706787, mean_q: 14.396832\n"," 4924/10000: episode: 252, duration: 0.221s, episode steps:  24, steps per second: 109, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3.129759, mae: 7.745958, mean_q: 14.507172\n"," 4948/10000: episode: 253, duration: 0.254s, episode steps:  24, steps per second:  95, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 3.429453, mae: 7.772681, mean_q: 14.526875\n"," 4964/10000: episode: 254, duration: 0.146s, episode steps:  16, steps per second: 110, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 3.782307, mae: 7.804967, mean_q: 14.574778\n"," 4983/10000: episode: 255, duration: 0.181s, episode steps:  19, steps per second: 105, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 1.742941, mae: 7.701394, mean_q: 14.604478\n"," 4997/10000: episode: 256, duration: 0.128s, episode steps:  14, steps per second: 109, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 3.355857, mae: 7.951764, mean_q: 14.923959\n"," 5018/10000: episode: 257, duration: 0.210s, episode steps:  21, steps per second: 100, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.810639, mae: 7.852825, mean_q: 14.648768\n"," 5061/10000: episode: 258, duration: 0.416s, episode steps:  43, steps per second: 103, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.631584, mae: 7.877179, mean_q: 14.859735\n"," 5105/10000: episode: 259, duration: 0.395s, episode steps:  44, steps per second: 111, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 3.377381, mae: 7.974696, mean_q: 14.931994\n"," 5141/10000: episode: 260, duration: 0.337s, episode steps:  36, steps per second: 107, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.217439, mae: 8.016397, mean_q: 15.088207\n"," 5154/10000: episode: 261, duration: 0.126s, episode steps:  13, steps per second: 103, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 3.138380, mae: 8.032246, mean_q: 15.077774\n"," 5180/10000: episode: 262, duration: 0.243s, episode steps:  26, steps per second: 107, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.593249, mae: 8.045703, mean_q: 15.047596\n"," 5203/10000: episode: 263, duration: 0.249s, episode steps:  23, steps per second:  92, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.348 [0.000, 1.000],  loss: 4.342020, mae: 8.164371, mean_q: 15.178460\n"," 5213/10000: episode: 264, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 3.530482, mae: 8.150440, mean_q: 15.200571\n"," 5227/10000: episode: 265, duration: 0.141s, episode steps:  14, steps per second:  99, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.727311, mae: 8.203174, mean_q: 15.236213\n"," 5254/10000: episode: 266, duration: 0.252s, episode steps:  27, steps per second: 107, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.630248, mae: 8.069670, mean_q: 15.040809\n"," 5272/10000: episode: 267, duration: 0.156s, episode steps:  18, steps per second: 115, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.542305, mae: 8.005695, mean_q: 15.145701\n"," 5359/10000: episode: 268, duration: 0.809s, episode steps:  87, steps per second: 108, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.855913, mae: 8.163902, mean_q: 15.242456\n"," 5403/10000: episode: 269, duration: 0.388s, episode steps:  44, steps per second: 113, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.180125, mae: 8.205613, mean_q: 15.403327\n"," 5424/10000: episode: 270, duration: 0.206s, episode steps:  21, steps per second: 102, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.308183, mae: 8.221019, mean_q: 15.427885\n"," 5440/10000: episode: 271, duration: 0.145s, episode steps:  16, steps per second: 110, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 3.826795, mae: 8.214957, mean_q: 15.401535\n"," 5476/10000: episode: 272, duration: 0.350s, episode steps:  36, steps per second: 103, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.074318, mae: 8.225483, mean_q: 15.479855\n"," 5503/10000: episode: 273, duration: 0.250s, episode steps:  27, steps per second: 108, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.626777, mae: 8.263300, mean_q: 15.490281\n"," 5523/10000: episode: 274, duration: 0.197s, episode steps:  20, steps per second: 101, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.608624, mae: 8.350039, mean_q: 15.622049\n"," 5541/10000: episode: 275, duration: 0.162s, episode steps:  18, steps per second: 111, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.217717, mae: 8.284533, mean_q: 15.527688\n"," 5553/10000: episode: 276, duration: 0.120s, episode steps:  12, steps per second: 100, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 3.951975, mae: 8.400273, mean_q: 15.756527\n"," 5564/10000: episode: 277, duration: 0.103s, episode steps:  11, steps per second: 107, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 4.200819, mae: 8.345036, mean_q: 15.457450\n"," 5626/10000: episode: 278, duration: 0.585s, episode steps:  62, steps per second: 106, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 3.439670, mae: 8.347079, mean_q: 15.651569\n"," 5639/10000: episode: 279, duration: 0.123s, episode steps:  13, steps per second: 106, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.704987, mae: 8.211796, mean_q: 15.639136\n"," 5693/10000: episode: 280, duration: 0.488s, episode steps:  54, steps per second: 111, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.672774, mae: 8.498091, mean_q: 15.952920\n"," 5724/10000: episode: 281, duration: 0.282s, episode steps:  31, steps per second: 110, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 3.348830, mae: 8.396146, mean_q: 15.851084\n"," 5808/10000: episode: 282, duration: 0.765s, episode steps:  84, steps per second: 110, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 3.705477, mae: 8.595156, mean_q: 16.161255\n"," 5819/10000: episode: 283, duration: 0.100s, episode steps:  11, steps per second: 110, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 2.548228, mae: 8.625854, mean_q: 16.329506\n"," 5829/10000: episode: 284, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 4.884808, mae: 8.830969, mean_q: 16.485134\n"," 5876/10000: episode: 285, duration: 0.426s, episode steps:  47, steps per second: 110, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 3.664088, mae: 8.701737, mean_q: 16.410597\n"," 5887/10000: episode: 286, duration: 0.104s, episode steps:  11, steps per second: 105, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 3.077564, mae: 8.541461, mean_q: 16.162388\n"," 5901/10000: episode: 287, duration: 0.150s, episode steps:  14, steps per second:  93, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.087021, mae: 8.645846, mean_q: 16.543201\n"," 5924/10000: episode: 288, duration: 0.229s, episode steps:  23, steps per second: 100, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 3.805647, mae: 8.825354, mean_q: 16.668694\n"," 5947/10000: episode: 289, duration: 0.201s, episode steps:  23, steps per second: 114, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.672655, mae: 8.738622, mean_q: 16.474579\n"," 5967/10000: episode: 290, duration: 0.183s, episode steps:  20, steps per second: 109, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.621567, mae: 8.843801, mean_q: 16.685053\n"," 6067/10000: episode: 291, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.193726, mae: 8.801947, mean_q: 16.529379\n"," 6081/10000: episode: 292, duration: 0.130s, episode steps:  14, steps per second: 107, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 3.896717, mae: 8.859469, mean_q: 16.676704\n"," 6117/10000: episode: 293, duration: 0.311s, episode steps:  36, steps per second: 116, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.409806, mae: 8.903260, mean_q: 16.810097\n"," 6192/10000: episode: 294, duration: 0.670s, episode steps:  75, steps per second: 112, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.465818, mae: 8.907535, mean_q: 16.919014\n"," 6220/10000: episode: 295, duration: 0.244s, episode steps:  28, steps per second: 115, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 4.900644, mae: 9.088075, mean_q: 17.050808\n"," 6273/10000: episode: 296, duration: 0.484s, episode steps:  53, steps per second: 109, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 4.411742, mae: 9.099044, mean_q: 17.100683\n"," 6330/10000: episode: 297, duration: 0.501s, episode steps:  57, steps per second: 114, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 3.625520, mae: 9.069659, mean_q: 17.101295\n"," 6372/10000: episode: 298, duration: 0.401s, episode steps:  42, steps per second: 105, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.133599, mae: 9.174273, mean_q: 17.329224\n"," 6395/10000: episode: 299, duration: 0.195s, episode steps:  23, steps per second: 118, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 3.187705, mae: 9.227182, mean_q: 17.522295\n"," 6438/10000: episode: 300, duration: 0.412s, episode steps:  43, steps per second: 104, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 3.687263, mae: 9.256851, mean_q: 17.528389\n"," 6464/10000: episode: 301, duration: 0.237s, episode steps:  26, steps per second: 110, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.629856, mae: 9.349115, mean_q: 17.691626\n"," 6518/10000: episode: 302, duration: 0.487s, episode steps:  54, steps per second: 111, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.580019, mae: 9.299495, mean_q: 17.644876\n"," 6558/10000: episode: 303, duration: 0.352s, episode steps:  40, steps per second: 114, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 4.477612, mae: 9.341454, mean_q: 17.604908\n"," 6617/10000: episode: 304, duration: 0.537s, episode steps:  59, steps per second: 110, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.493903, mae: 9.421535, mean_q: 17.788256\n"," 6646/10000: episode: 305, duration: 0.249s, episode steps:  29, steps per second: 117, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.704828, mae: 9.508248, mean_q: 18.013247\n"," 6671/10000: episode: 306, duration: 0.244s, episode steps:  25, steps per second: 102, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.168069, mae: 9.437955, mean_q: 17.862228\n"," 6689/10000: episode: 307, duration: 0.166s, episode steps:  18, steps per second: 108, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.513520, mae: 9.486588, mean_q: 18.038282\n"," 6716/10000: episode: 308, duration: 0.258s, episode steps:  27, steps per second: 105, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.848480, mae: 9.549815, mean_q: 18.014597\n"," 6745/10000: episode: 309, duration: 0.260s, episode steps:  29, steps per second: 111, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.059675, mae: 9.563307, mean_q: 18.244585\n"," 6783/10000: episode: 310, duration: 0.333s, episode steps:  38, steps per second: 114, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.016955, mae: 9.548393, mean_q: 18.119368\n"," 6808/10000: episode: 311, duration: 0.241s, episode steps:  25, steps per second: 104, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.740710, mae: 9.580764, mean_q: 18.214172\n"," 6842/10000: episode: 312, duration: 0.302s, episode steps:  34, steps per second: 113, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.810769, mae: 9.634970, mean_q: 18.417912\n"," 6874/10000: episode: 313, duration: 0.296s, episode steps:  32, steps per second: 108, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.403349, mae: 9.744038, mean_q: 18.558878\n"," 6891/10000: episode: 314, duration: 0.151s, episode steps:  17, steps per second: 112, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5.393317, mae: 9.796165, mean_q: 18.524612\n"," 6906/10000: episode: 315, duration: 0.141s, episode steps:  15, steps per second: 106, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.270277, mae: 9.739245, mean_q: 18.520714\n"," 6959/10000: episode: 316, duration: 0.504s, episode steps:  53, steps per second: 105, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 3.802191, mae: 9.767565, mean_q: 18.588171\n"," 6979/10000: episode: 317, duration: 0.186s, episode steps:  20, steps per second: 107, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.720908, mae: 9.830485, mean_q: 18.688114\n"," 7041/10000: episode: 318, duration: 0.571s, episode steps:  62, steps per second: 109, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 4.472410, mae: 9.881536, mean_q: 18.717457\n"," 7082/10000: episode: 319, duration: 0.353s, episode steps:  41, steps per second: 116, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 4.235365, mae: 9.941932, mean_q: 18.914297\n"," 7111/10000: episode: 320, duration: 0.254s, episode steps:  29, steps per second: 114, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 4.722411, mae: 9.974164, mean_q: 18.915678\n"," 7163/10000: episode: 321, duration: 0.477s, episode steps:  52, steps per second: 109, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.667551, mae: 9.986481, mean_q: 18.928722\n"," 7184/10000: episode: 322, duration: 0.190s, episode steps:  21, steps per second: 110, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.918151, mae: 10.019445, mean_q: 19.050362\n"," 7213/10000: episode: 323, duration: 0.269s, episode steps:  29, steps per second: 108, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 3.577287, mae: 9.971978, mean_q: 19.069990\n"," 7230/10000: episode: 324, duration: 0.159s, episode steps:  17, steps per second: 107, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.331861, mae: 10.130219, mean_q: 19.455631\n"," 7272/10000: episode: 325, duration: 0.381s, episode steps:  42, steps per second: 110, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 5.315766, mae: 10.145922, mean_q: 19.282055\n"," 7292/10000: episode: 326, duration: 0.193s, episode steps:  20, steps per second: 104, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 6.557706, mae: 10.255626, mean_q: 19.280853\n"," 7350/10000: episode: 327, duration: 0.514s, episode steps:  58, steps per second: 113, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 4.122130, mae: 10.106238, mean_q: 19.258503\n"," 7372/10000: episode: 328, duration: 0.217s, episode steps:  22, steps per second: 101, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.345928, mae: 10.197598, mean_q: 19.512564\n"," 7386/10000: episode: 329, duration: 0.119s, episode steps:  14, steps per second: 117, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.936567, mae: 10.158671, mean_q: 19.387836\n"," 7466/10000: episode: 330, duration: 0.714s, episode steps:  80, steps per second: 112, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 4.484830, mae: 10.242465, mean_q: 19.521748\n"," 7524/10000: episode: 331, duration: 0.544s, episode steps:  58, steps per second: 107, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 4.095885, mae: 10.295225, mean_q: 19.672878\n"," 7538/10000: episode: 332, duration: 0.128s, episode steps:  14, steps per second: 109, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 5.634841, mae: 10.383338, mean_q: 19.579441\n"," 7634/10000: episode: 333, duration: 0.829s, episode steps:  96, steps per second: 116, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 4.936598, mae: 10.405955, mean_q: 19.727221\n"," 7646/10000: episode: 334, duration: 0.115s, episode steps:  12, steps per second: 104, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 4.790537, mae: 10.467605, mean_q: 19.831732\n"," 7677/10000: episode: 335, duration: 0.272s, episode steps:  31, steps per second: 114, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 6.168165, mae: 10.442935, mean_q: 19.718369\n"," 7752/10000: episode: 336, duration: 0.659s, episode steps:  75, steps per second: 114, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 4.377691, mae: 10.460336, mean_q: 19.940403\n"," 7788/10000: episode: 337, duration: 0.318s, episode steps:  36, steps per second: 113, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.850617, mae: 10.503803, mean_q: 20.154293\n"," 7805/10000: episode: 338, duration: 0.160s, episode steps:  17, steps per second: 106, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.359517, mae: 10.510130, mean_q: 20.116062\n"," 7822/10000: episode: 339, duration: 0.160s, episode steps:  17, steps per second: 106, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 2.801554, mae: 10.622128, mean_q: 20.517237\n"," 7857/10000: episode: 340, duration: 0.332s, episode steps:  35, steps per second: 105, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 4.906876, mae: 10.694238, mean_q: 20.457075\n"," 7943/10000: episode: 341, duration: 0.770s, episode steps:  86, steps per second: 112, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.854841, mae: 10.615601, mean_q: 20.244444\n"," 7960/10000: episode: 342, duration: 0.180s, episode steps:  17, steps per second:  95, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.808917, mae: 10.833096, mean_q: 20.812109\n"," 8017/10000: episode: 343, duration: 0.538s, episode steps:  57, steps per second: 106, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 5.017296, mae: 10.895030, mean_q: 20.784615\n"," 8040/10000: episode: 344, duration: 0.213s, episode steps:  23, steps per second: 108, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.560896, mae: 10.852091, mean_q: 20.881226\n"," 8087/10000: episode: 345, duration: 0.427s, episode steps:  47, steps per second: 110, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 4.201343, mae: 10.865488, mean_q: 20.825033\n"," 8106/10000: episode: 346, duration: 0.168s, episode steps:  19, steps per second: 113, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 6.263439, mae: 10.976181, mean_q: 20.757681\n"," 8188/10000: episode: 347, duration: 0.707s, episode steps:  82, steps per second: 116, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.909706, mae: 10.958522, mean_q: 20.937876\n"," 8209/10000: episode: 348, duration: 0.182s, episode steps:  21, steps per second: 116, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 4.787906, mae: 11.058281, mean_q: 21.147453\n"," 8250/10000: episode: 349, duration: 0.380s, episode steps:  41, steps per second: 108, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 5.681833, mae: 11.023522, mean_q: 20.965649\n"," 8301/10000: episode: 350, duration: 0.448s, episode steps:  51, steps per second: 114, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.160028, mae: 11.057429, mean_q: 21.149946\n"," 8320/10000: episode: 351, duration: 0.180s, episode steps:  19, steps per second: 106, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 4.179823, mae: 11.218896, mean_q: 21.500198\n"," 8355/10000: episode: 352, duration: 0.314s, episode steps:  35, steps per second: 112, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.323940, mae: 11.147996, mean_q: 21.355806\n"," 8408/10000: episode: 353, duration: 0.463s, episode steps:  53, steps per second: 115, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 4.927707, mae: 11.198924, mean_q: 21.382404\n"," 8426/10000: episode: 354, duration: 0.173s, episode steps:  18, steps per second: 104, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.855045, mae: 11.132730, mean_q: 21.429773\n"," 8480/10000: episode: 355, duration: 0.459s, episode steps:  54, steps per second: 118, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.119002, mae: 11.254699, mean_q: 21.475121\n"," 8496/10000: episode: 356, duration: 0.162s, episode steps:  16, steps per second:  99, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 6.450768, mae: 11.405149, mean_q: 21.629885\n"," 8557/10000: episode: 357, duration: 0.554s, episode steps:  61, steps per second: 110, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 5.618926, mae: 11.268435, mean_q: 21.462034\n"," 8594/10000: episode: 358, duration: 0.344s, episode steps:  37, steps per second: 107, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 6.417015, mae: 11.426409, mean_q: 21.711758\n"," 8661/10000: episode: 359, duration: 0.622s, episode steps:  67, steps per second: 108, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 4.048803, mae: 11.356830, mean_q: 21.832052\n"," 8709/10000: episode: 360, duration: 0.430s, episode steps:  48, steps per second: 112, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 4.236426, mae: 11.456136, mean_q: 21.991980\n"," 8810/10000: episode: 361, duration: 0.922s, episode steps: 101, steps per second: 109, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 4.157199, mae: 11.479701, mean_q: 22.097664\n"," 8886/10000: episode: 362, duration: 0.654s, episode steps:  76, steps per second: 116, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 5.138147, mae: 11.733525, mean_q: 22.528923\n"," 8928/10000: episode: 363, duration: 0.395s, episode steps:  42, steps per second: 106, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 4.844312, mae: 11.694402, mean_q: 22.451792\n"," 8990/10000: episode: 364, duration: 0.551s, episode steps:  62, steps per second: 113, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.884943, mae: 11.764147, mean_q: 22.604355\n"," 9054/10000: episode: 365, duration: 0.629s, episode steps:  64, steps per second: 102, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.670187, mae: 11.803624, mean_q: 22.737957\n"," 9090/10000: episode: 366, duration: 0.338s, episode steps:  36, steps per second: 107, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.417161, mae: 11.888423, mean_q: 22.799955\n"," 9112/10000: episode: 367, duration: 0.201s, episode steps:  22, steps per second: 109, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.582557, mae: 11.839361, mean_q: 22.854755\n"," 9163/10000: episode: 368, duration: 0.476s, episode steps:  51, steps per second: 107, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 5.975717, mae: 12.031887, mean_q: 23.043211\n"," 9236/10000: episode: 369, duration: 0.664s, episode steps:  73, steps per second: 110, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 5.017076, mae: 12.036833, mean_q: 23.129045\n"," 9268/10000: episode: 370, duration: 0.296s, episode steps:  32, steps per second: 108, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 7.600579, mae: 12.091923, mean_q: 23.040989\n"," 9332/10000: episode: 371, duration: 0.573s, episode steps:  64, steps per second: 112, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 6.083241, mae: 12.122135, mean_q: 23.181610\n"," 9352/10000: episode: 372, duration: 0.192s, episode steps:  20, steps per second: 104, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 5.021918, mae: 12.053373, mean_q: 23.113976\n"," 9376/10000: episode: 373, duration: 0.207s, episode steps:  24, steps per second: 116, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.844839, mae: 12.112419, mean_q: 23.351831\n"," 9436/10000: episode: 374, duration: 0.539s, episode steps:  60, steps per second: 111, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 4.615320, mae: 12.190176, mean_q: 23.499693\n"," 9454/10000: episode: 375, duration: 0.165s, episode steps:  18, steps per second: 109, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 7.071381, mae: 12.324577, mean_q: 23.527843\n"," 9470/10000: episode: 376, duration: 0.155s, episode steps:  16, steps per second: 103, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.097097, mae: 12.424666, mean_q: 23.625446\n"," 9511/10000: episode: 377, duration: 0.382s, episode steps:  41, steps per second: 107, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 5.996265, mae: 12.443005, mean_q: 23.855879\n"," 9567/10000: episode: 378, duration: 0.529s, episode steps:  56, steps per second: 106, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 5.812607, mae: 12.325921, mean_q: 23.641842\n"," 9638/10000: episode: 379, duration: 0.662s, episode steps:  71, steps per second: 107, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 7.127714, mae: 12.503665, mean_q: 23.909803\n"," 9690/10000: episode: 380, duration: 0.482s, episode steps:  52, steps per second: 108, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.937603, mae: 12.516621, mean_q: 23.957867\n"," 9710/10000: episode: 381, duration: 0.182s, episode steps:  20, steps per second: 110, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.387149, mae: 12.563687, mean_q: 23.893522\n"," 9750/10000: episode: 382, duration: 0.361s, episode steps:  40, steps per second: 111, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.698743, mae: 12.567554, mean_q: 24.232519\n"," 9831/10000: episode: 383, duration: 0.743s, episode steps:  81, steps per second: 109, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.681799, mae: 12.579896, mean_q: 24.210714\n"," 9865/10000: episode: 384, duration: 0.321s, episode steps:  34, steps per second: 106, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 8.029106, mae: 12.732788, mean_q: 24.271046\n"," 9902/10000: episode: 385, duration: 0.341s, episode steps:  37, steps per second: 108, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 5.353369, mae: 12.634995, mean_q: 24.315058\n"," 9919/10000: episode: 386, duration: 0.155s, episode steps:  17, steps per second: 110, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 7.833214, mae: 12.800217, mean_q: 24.380251\n"," 9961/10000: episode: 387, duration: 0.369s, episode steps:  42, steps per second: 114, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.130709, mae: 12.691858, mean_q: 24.314157\n","done, took 94.688 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f0330569580>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["score = dqn.test(env,nb_episodes=100,visualize=False)\n","np.mean(score.history['episode_reward'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLCmyLtycYJ4","executionInfo":{"status":"ok","timestamp":1671589762798,"user_tz":-330,"elapsed":8741,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}},"outputId":"e8ecb7ee-eef6-4fe0-a634-1c3b9bd3e15f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing for 100 episodes ...\n","Episode 1: reward: 65.000, steps: 65\n","Episode 2: reward: 62.000, steps: 62\n","Episode 3: reward: 65.000, steps: 65\n","Episode 4: reward: 53.000, steps: 53\n","Episode 5: reward: 52.000, steps: 52\n","Episode 6: reward: 55.000, steps: 55\n","Episode 7: reward: 79.000, steps: 79\n","Episode 8: reward: 63.000, steps: 63\n","Episode 9: reward: 106.000, steps: 106\n","Episode 10: reward: 63.000, steps: 63\n","Episode 11: reward: 51.000, steps: 51\n","Episode 12: reward: 52.000, steps: 52\n","Episode 13: reward: 53.000, steps: 53\n","Episode 14: reward: 58.000, steps: 58\n","Episode 15: reward: 75.000, steps: 75\n","Episode 16: reward: 49.000, steps: 49\n","Episode 17: reward: 71.000, steps: 71\n","Episode 18: reward: 64.000, steps: 64\n","Episode 19: reward: 102.000, steps: 102\n","Episode 20: reward: 58.000, steps: 58\n","Episode 21: reward: 68.000, steps: 68\n","Episode 22: reward: 77.000, steps: 77\n","Episode 23: reward: 57.000, steps: 57\n","Episode 24: reward: 58.000, steps: 58\n","Episode 25: reward: 82.000, steps: 82\n","Episode 26: reward: 96.000, steps: 96\n","Episode 27: reward: 92.000, steps: 92\n","Episode 28: reward: 51.000, steps: 51\n","Episode 29: reward: 107.000, steps: 107\n","Episode 30: reward: 76.000, steps: 76\n","Episode 31: reward: 56.000, steps: 56\n","Episode 32: reward: 42.000, steps: 42\n","Episode 33: reward: 52.000, steps: 52\n","Episode 34: reward: 51.000, steps: 51\n","Episode 35: reward: 64.000, steps: 64\n","Episode 36: reward: 53.000, steps: 53\n","Episode 37: reward: 48.000, steps: 48\n","Episode 38: reward: 62.000, steps: 62\n","Episode 39: reward: 172.000, steps: 172\n","Episode 40: reward: 66.000, steps: 66\n","Episode 41: reward: 57.000, steps: 57\n","Episode 42: reward: 62.000, steps: 62\n","Episode 43: reward: 70.000, steps: 70\n","Episode 44: reward: 49.000, steps: 49\n","Episode 45: reward: 48.000, steps: 48\n","Episode 46: reward: 132.000, steps: 132\n","Episode 47: reward: 46.000, steps: 46\n","Episode 48: reward: 105.000, steps: 105\n","Episode 49: reward: 170.000, steps: 170\n","Episode 50: reward: 69.000, steps: 69\n","Episode 51: reward: 166.000, steps: 166\n","Episode 52: reward: 78.000, steps: 78\n","Episode 53: reward: 115.000, steps: 115\n","Episode 54: reward: 57.000, steps: 57\n","Episode 55: reward: 51.000, steps: 51\n","Episode 56: reward: 53.000, steps: 53\n","Episode 57: reward: 53.000, steps: 53\n","Episode 58: reward: 47.000, steps: 47\n","Episode 59: reward: 177.000, steps: 177\n","Episode 60: reward: 77.000, steps: 77\n","Episode 61: reward: 82.000, steps: 82\n","Episode 62: reward: 155.000, steps: 155\n","Episode 63: reward: 51.000, steps: 51\n","Episode 64: reward: 55.000, steps: 55\n","Episode 65: reward: 122.000, steps: 122\n","Episode 66: reward: 139.000, steps: 139\n","Episode 67: reward: 50.000, steps: 50\n","Episode 68: reward: 83.000, steps: 83\n","Episode 69: reward: 200.000, steps: 200\n","Episode 70: reward: 69.000, steps: 69\n","Episode 71: reward: 55.000, steps: 55\n","Episode 72: reward: 91.000, steps: 91\n","Episode 73: reward: 55.000, steps: 55\n","Episode 74: reward: 200.000, steps: 200\n","Episode 75: reward: 47.000, steps: 47\n","Episode 76: reward: 84.000, steps: 84\n","Episode 77: reward: 188.000, steps: 188\n","Episode 78: reward: 52.000, steps: 52\n","Episode 79: reward: 56.000, steps: 56\n","Episode 80: reward: 63.000, steps: 63\n","Episode 81: reward: 56.000, steps: 56\n","Episode 82: reward: 44.000, steps: 44\n","Episode 83: reward: 45.000, steps: 45\n","Episode 84: reward: 49.000, steps: 49\n","Episode 85: reward: 55.000, steps: 55\n","Episode 86: reward: 135.000, steps: 135\n","Episode 87: reward: 54.000, steps: 54\n","Episode 88: reward: 190.000, steps: 190\n","Episode 89: reward: 89.000, steps: 89\n","Episode 90: reward: 55.000, steps: 55\n","Episode 91: reward: 200.000, steps: 200\n","Episode 92: reward: 53.000, steps: 53\n","Episode 93: reward: 55.000, steps: 55\n","Episode 94: reward: 73.000, steps: 73\n","Episode 95: reward: 64.000, steps: 64\n","Episode 96: reward: 94.000, steps: 94\n","Episode 97: reward: 65.000, steps: 65\n","Episode 98: reward: 49.000, steps: 49\n","Episode 99: reward: 68.000, steps: 68\n","Episode 100: reward: 46.000, steps: 46\n"]},{"output_type":"execute_result","data":{"text/plain":["78.44"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[],"metadata":{"id":"84u1Cv52fcBq","executionInfo":{"status":"aborted","timestamp":1671589545857,"user_tz":-330,"elapsed":10,"user":{"displayName":"PRAHALAD VIJAYKUMAR","userId":"09373684625413507603"}}},"execution_count":null,"outputs":[]}]}